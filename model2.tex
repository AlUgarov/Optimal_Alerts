\documentclass[12pt,a4paper]{article}
\usepackage {amsmath,amsthm}
\usepackage{geometry}
\usepackage{titlesec}
 \geometry{a4paper, total={170mm,257mm}, left=20mm, right=20mm, top=25mm, bottom=25mm}
%\usepackage{tabularx}
\usepackage{sectsty}
\usepackage{natbib}
\bibliographystyle{econ}
\usepackage[hidelinks]{hyperref}
%\usepackage[colorlinks=true, urlcolor=Blue]{hyperref}
%\usepackage{titling}
\sectionfont{\fontsize{14}{15}\selectfont}
\subsectionfont{\fontsize{13}{15}\selectfont}

\titleformat*{\section}{\LARGE\bfseries}
\titleformat*{\subsection}{\Large\bfseries}


\usepackage{caption}
\captionsetup{belowskip=12pt,aboveskip=0pt}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother


\usepackage[dvipsnames]{xcolor}

\newcommand{\link}[1]{{\color{blue}\href{#1}{#1}}}


\usepackage[pdftex]{graphicx}
%\usepackage{cite}
\usepackage{booktabs}
\usepackage{float}
\usepackage{setspace}
\usepackage{placeins}
\usepackage{subcaption}

\newtheorem{theorem}{Proposition}

\begin{document}
\onehalfspacing
\section{Introduction}

Many real-life situations involve  designing or choosing tests to prevent a negative outcome inluding diagnostic medical tests, fire alarms and extreme weather alerts. These test almost never give perfectly information and hence their designers often face a trade-off between false-positive and false-negative outcomes. Still there is little empirical evidence on how users evaluate tests with different false-positive and false-negative rates. 

What is the optimal design of information on potential threats? We study demand for information in the framework with a potential protection action. The subject, first, receives a signal about the probability of an adverse event and then decides whether to protect or not. This environment describes several practically important scenarios including weather alerts, diagnostical testing and research investments in catastrophe prevention. 

Our setup directly translates to applications in two large areas:
\begin{itemize}
\item First, it describes signals about preventable negative events, such as medical tests, hazardous weather alerts and background checks. As patients have to make their own deicisions on taking some diagnostial medical tests and paying for them (mammograms), this experiment would help to identify features affecting the demand for testing. Most tests face the trade-off between true positive rate and false positive rate and this experiment would describe this trade-off in terms of subject's willingness-to-pay for a test.  The trade-off between false positive and true positives (or false alarm rate and probability of detection) is also a contentious topic in extreme weather forecasting (for example, \citet{simmons_false_2009}).

\item Second, this experiment would help to identify potential biases affecting our research on catastrophe prevention.  Fan, Jamison and Summers (2016) find that the expected annual costs of pandemics constitute around 0.7\% of world's GDP which is comparable to expected annual costs of global warming. Pandemics are largely unpredictable and their risks seems to be independent over time. However, the research on pandemics follows a strong cyclical pattern with more investment coming immediately after each large epidemic. This indicates a potential inefficiency in research investment which can happen for other potential catastrophes. Demand for information in our experiment parallels the demand for information on catastrophe prevention as long as catastrophes are low probability events with large potential losses for which a costly but certain prevention policy is available.
\end{itemize}

We find that despite certain clear biases the value of information in our setup correlates with the willingness-to-pay. ---

Our paper is the first to measure value of information in the setting of diagnostic tests or alarms. Previous work finds that tests ---

Our work is one of a few empirical studies measuring demand for information used for decision-making (instrumental information). The field experiment conducted by \citep{hoffman_how_2016} finds that the demand for information increases with initial uncertainty, but decreases with the signal's accuracy. However, the decrease in accuracy is more modest than expected for a Bayesian decision-maker resulting in subjects underpaying for high-quality signals.  \citet{ambuehl_belief_2018} also study the demand for information in the setup in which individuals receive a positive payoff when they correctly predict the state of the world based on a signal. They find that subjects tend to underreact to value's accuracy, but put a premium on completely certain signals. 

Our setup differs in two important aspects from \citet{ambuehl_belief_2018}, because we study alerts and not prediction tasks. First, we explore the effect of prior probabilities and hence one state usually have a higher probability than the other. Second, the subject faces a costly protection decision and not a prediction decisions, resulting in three distinct payoffs: full payoff, full payoff minus protection costs and full payoff minus losses. Consistent with \citet{ambuehl_belief_2018} we also find that subjects undervalue accurate signals, but we do not find a premium for certain signals. 

This work also relates to the vast literature on demand for insurance and protection. Similar to our findings, several studies observe that the demand for insurance goes up after the recent experience with low-probabiity events. Field evidence indicates that people underinsure with respect to rare natural disasters (Friedl et al, 2014). \citet{laury_insurance_2009} find no under-insurance for low-probability events in the laboratory setting. One offered explanation \citep{volkman-wise_representativeness_2015} is that subjects overweight recent evidence leading to underinsurance when there were no negative events in the recent past and to overinsurance after the fact. It is consistent with underweighting prior probabilities relative to more recent signals. 

The bias we are finding is similar to the base-rate and signal neglect phenomenons. Psychology researchers \citet{hammerton_case_1973} and \citet{kahneman_psychology_1973} first observe that subjects underweight prior probabilities (base rates) when calculating posteriors. This phenomenon receives the name of \textit{base-rate neglect}. Multiple studies in economics then confirm \citep*{grether_testing_1992, holt_update_2009} this phenomenon in incentivized laboratory experiments. These studies also find that subjects not only underweight priors but also underweight signals they receive.  We observe both phenomenons in responses to our belief elicitation task, but the calculation of signals' values differs substantially from the calculation of posterior probabilities. While the calculation of posterior probabilities would require using a Bayes formula, signal's value depends only on products of prior probabilities. However, we observe that subjects underestimate the effect of priors compared to theoretical predictions for an expected-utility decision-maker.


\vspace{20pt}

\section{Model}
\vspace{10pt}
\bf Environment. \rm The model describes a decision-maker considering a purchase of threat-assessment information. Let $\omega \in \{0,1\}$ denote the state of world, where 1 corresponds to some adverse event happening with probability $\pi$. The decision-maker has a lower utility in the adverse state, but only if she does not take the protective action. Denote actions by $a\in\{0,1\}$, where 1 means taking the protective action. The protection technology is perfect: protected agents bear no losses but pay protection costs $c$ regardless of the state $\omega$. Decision-maker preferences are described by the utility which depends on wealth $Y$, protective action $a$ and potential damage in the adverse state $\omega(1-a)$. Utility is separable in wealth, protection costs $c$ and potential loss in the adverse state $L$:
\begin{equation}
U=U(Y,a,\omega(1-a))=u(Y-ac-\omega(1-a)L)
\end{equation}

Separability condition is ---

The decision-maker can purchase a binary informative signal $s\in\{0,1\}$ about the state of the world before making a decision. Let $P_{ij}\equiv P(s=i|\omega=j)$ be the probability of a signal taking value $i$ conditional on the state of the world being $j$.  After receiving the signal, the decision-maker updates her belief on the likelihood of the bad state to $\mu(s)$. Unless specified otherwise, we assume that the decision-maker forms her posterior beliefs by using the Bayes rule. Hence the posterior belief equals:
\begin{equation}
\mu(s)= {\pi P_{s1} \over \pi P_{s1}+(1-\pi)P_{s0}}
\end{equation}

We also assume without loss of generality that a higher signal means a higher posterior probability of an adverse event $\mu(1)\geq\mu(0)$. Otherwise we can always re-label the signals.

This environment can describe fire alarm systems, medical diagnostic tests for preventable or treatable conditions, equipment malfunctioning sensors, and extreme weather alerts. We abstract from the non-instrumental dimension of information which is explored in many other recent studies. These studies observe that information can be not relevant for subsequent decisions but relevant for the decision-maker if it affects personal beliefs or expectations of future events.  Hence our results should apply more to situations with relatively little time between the signal and resolution of uncertainty and relatively low emotional involvement.

\vspace{10pt}
\bf Preferences. \rm If there is no signal, the decision-maker protects if and only if it increases their expected utility:
\begin{equation}
EU_0=\max[u(Y-c),\pi u(Y-L)+(1-\pi) u(Y)]
\end{equation}
The signal can increase expected utility if the decision-maker reacts differently to positive and negative signals. Under these assumptions, her expected utility with a signal is:
\begin{equation}
EU_s=\pi P_{11}u(Y-c)+\pi P_{01}u(Y-L)+(1-\pi)P_{10}u(Y-c)+(1-\pi)P_{00}u(Y)
\end{equation}

We consider the maximum amount $b$ which the decision-maker is willing to pay for the signal. In our framework, it is a price paid with a signal such that a decision-maker is indifferent between having a signal and paying $b$ and not having a signal. Because the decision-maker can always ignore a useless signal, the signal's value is bounded from below by zero. Hence it equals to the maximum between zero and the solution to the following equation:
\begin{equation}
\begin{split}
P(s=1)u(Y-b-c)+\pi P_{01}u(Y-b-L)+(1-\pi)P_{00}u(Y-b)=\\=\max[u(Y-c),\pi u(Y-L)+(1-\pi) u(Y)] 
\end{split}
\end{equation}

The left-hand side expression of this equation is a strictly decreasing function of $b$. Additionally, for $b\rightarrow \infty$ the left-hand side is smaller than the right-hand side. It implies that the equation (5) above has a at most one positive solution.

Perfectly accurate signals always have some positive value $b>0$ because the payoff distribution with the signal first-order stochastically dominates the distribution without the signal. Imperfect signals have positive value if they are informative ($P_{01}<P_{00}$) and the decision-maker doesn't protect without the signal. Depending on preferences and signal characteristics, imperfect but informative signals can also have zero value. 

\vspace{10pt}
\bf Risk-neutral agent. \rm If the decision-maker is risk-neutral, the expression above collapses to:
$$b+P(s=1)c+\pi P_{01}L=\min[c,\pi L]$$

The signal's value is just:
\begin{equation}
b=\max[0,\min[c,\pi L]-P(s=1)c-\pi P_{01}L]
\end{equation}

We can express WTP $b$ as a function of priors, false-positive and false-negative rates. This is the equation we use in our empirical work:
\begin{equation}
b=\max[0,\min[c,\pi L]-\pi (1-P_{01})c-(1-\pi)P_{10}c-\pi P_{01}L]
\end{equation}

The sensitivity of (positive) value $b$ with respect to false-positive and false-negative rates is given by:
\begin{equation}
{db\over dP_{10}}=-(1-\pi)c
\end{equation}

\begin{equation}
{db\over dP_{10}}=-\pi(L-c)
\end{equation}
\vspace{10pt}

Both false-positive and false-negative rates decrease the (positive) signal's value. The effect is proportional to the adverse state probability for the false-negative rate and to the non-adverse state probability for the false-positive rates.

\vspace{10pt}
\bf Risk Aversion Effects. \rm In a more general expected utility framework, risk aversion can both increase and decrease the signal's value. More specifically, risk aversion decreases the value when the protection costs are low: 

\begin{theorem}
 If protection costs are low $c<\pi L$, then the strictly risk-averse decision-maker pays less than a risk-neutral one.
\end{theorem} 

It is harder to make definite statements for lower risks or higher protection costs. For example, risk aversion increases value of a perfect signal as long as risk-averse decision-maker still chooses to not protect without a signal. This follows from the standard argument of increasing demand for insurance with risk aversion and the fact that the protection problem with a perfect signal is isomorphic to the insurance problem with deductible $c$. 

Next, we study the effect of false-positive and false-negative rates on the signal's value $b$. Assuming a differentiable utility function $u()$ we use implicit differentiation to derive sensitivities of WTP $b$ to false-positive and false-negative rates:

$${db\over dP_{10}}=-{(1-\pi)(u(Y-b)-u(Y-c-b))\over D(\pi, P_{01}, P_{10}, b)}$$
$${db\over dP_{01}}=-{\pi(u(Y-c-b)-u(Y-L-b))\over D(\pi, P_{01}, P_{10}, b)}$$

With the denominator equal to the expected marginal utility:
$$D(\pi, P_{01}, P_{10}, b)\equiv P(S=1)u'(Y-c-b)+\pi P_{01}u'(Y-L-b)+$$
$$+(1-\pi)P_{00}u'(Y-b)>0$$

It is clear that the signal's value decreases with false-positive and false-negative rates ${db\over dP_{10}},{db\over dP_{01}}<0$. Moreover, any risk-averse decision-maker is more sensitive to false-negative rates than a risk-neutral one. To see why, first, note that the expected payoff for any signal with positive value belongs to the interval between $Y-c-b$ and $Y-b$. Next, use the mean value theorem to rewrite that sensitivity as:

$${db\over dP_{01}}=-{\pi u'(\zeta)(L-c)\over E[MU]},\zeta \in \left(Y-c -b, Y-b\right)$$

For any strictly risk-averse subject $u'(\zeta)<E[MU]$  and hence ${db\over dP_{01}}<-\pi(L-c)$. The opposite is true for a strictly risk-averse subject.

However, a risk-aversion can both increase and decrease subject's sensitivity to FP rates depending on the utility function curvature and signal's characteristics. Intuitively, an expected marginal utility of a strongly risk-averse subject with a bad signal can be lower than the average slope of the utility function between $Y-c-b$ and $Y-b$ reducing sensitivity to false-positive rates. It can also be higher if either the signal is good or the curvature is small. 

\newpage
\singlespacing
\small

\bibliography{Alerts}


 
\appendix
\newpage
\section{Proofs}

Proof of Proposition 1: If protection costs are low enough $c<\pi L$ than the risk-neutral decision-maker should always protect without a signal:
$$U=\max[\pi(Y-L)+(1-\pi)Y,Y-c]=Y-c$$

It means that a strictly risk-averse decision-maker with a utility function $u()$ should also protect:
$$\pi u(Y-L)+(1-\pi)u(Y)<u(\pi(Y-L)+(1-\pi)Y)=u(Y-c)$$

Then denote stochastic payoff with a signal as $X$ so that expected utility with a signal is $Eu(X-b)$ where $b$ is the willingness-to-pay solving:
$$Eu(X-b)=u(Y-c)$$
 Let $b_0$ be the willingness-to-pay for a risk-neutral decision-maker. By Jensen's inequality:
$$Eu(X-b_0)<u(EX-b_0)=u(Y-c)=Eu(X-b)$$

Because expected utility with a signal is a decreasing function of $b_0$ we obtain $b>b_0$.


\end{document}